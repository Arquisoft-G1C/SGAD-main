Escenario 1 â€” Reverse Proxy falla o entra en timeout

(Lo completamos cuando me confirmes quÃ© quieres medir)

Escenario 2 â€” CaÃ­da de una instancia del API Gateway

â†’ Sistema debe seguir funcionando con las otras 3 rÃ©plicas.

Escenario 3 â€” CaÃ­da de un microservicio crÃ­tico (Auth Service)

â†’ El sistema debe manejar â€œfail-stopâ€ y responder con error controlado.

Escenario 4 â€” Fallo de base de datos / retry + degradaciÃ³n controlada

â†’ Un microservicio no puede conectarse al Postgres/Mongo, debe reintentar con backoff.



ğŸ”¹ Escenario 1 â€“ Falla del Reverse Proxy (NGINX) por â€œcongelamientoâ€ (pause)
1. DefiniciÃ³n del escenario

Source (fuente): Cliente externo (curl desde el equipo del usuario).

Stimulus (estÃ­mulo): El reverse proxy se congela (estado paused en Docker), dejando de procesar peticiones.

Environment (entorno): Sistema SGAD desplegado con todos los contenedores en estado Up, recibiendo trÃ¡fico HTTP/HTTPS.

Artifact (artefacto): Contenedor sgad-reverse-proxy (NGINX).

Response (respuesta esperada):

Antes de la falla: el reverse proxy enruta correctamente las peticiones al backend.

Durante la falla: las peticiones del cliente quedan â€œcolgadasâ€ (sin respuesta) porque el reverse proxy estÃ¡ congelado.

El backend (API Gateway y microservicios) sigue operativo internamente.

Tras reanudar el contenedor, el sistema vuelve a responder sin reiniciar los demÃ¡s servicios.

Response Measure (medida):

ObservaciÃ³n del comportamiento de las requests (responden 307 vs. se quedan esperando).

VerificaciÃ³n de que solo el reverse proxy se ve afectado.

ConfirmaciÃ³n de que, al reanudar, el sistema vuelve al estado normal.

2. Paso a paso con los resultados obtenidos
âœ… Paso 1 â€“ VerificaciÃ³n del estado normal del sistema

AcciÃ³n:

Ejecutaste:

curl.exe -k -I "https://localhost/api/referees"


Resultado obtenido:

Respuestas tipo:

HTTP/1.1 307 Temporary Redirect
Server: nginx/1.28.0
Content-Length: 0
Connection: keep-alive
location: http://referee-service:3004/referees/
X-Upstream-Server: 172.18.0.2:8080
X-Load-Balancer: nginx-round-robin
Access-Control-Allow-Origin: *
...


InterpretaciÃ³n:

El reverse proxy NGINX estÃ¡ activo y responde correctamente.

El header X-Load-Balancer: nginx-round-robin confirma que el balanceador de carga estÃ¡ funcionando.

La cabecera location: http://referee-service:3004/referees/ muestra que la peticiÃ³n se redirige correctamente hacia el microservicio de Ã¡rbitros.

Esta salida sirve como estado base estable antes de inducir la falla.

âœ… Paso 2 â€“ EnvÃ­o de trÃ¡fico continuo (30 peticiones)

AcciÃ³n:

Ejecutaste:

for ($i=0; $i -lt 30; $i++) {
    try {
        curl.exe -k -I "https://localhost/api/referees"
    } catch {
        Write-Host "Error en request $i"
    }
}


Resultado obtenido:

Todas las respuestas fueron similares a:

HTTP/1.1 307 Temporary Redirect
Server: nginx/1.28.0
Date: Tue, 25 Nov 2025 01:12:24 GMT
Content-Length: 0
Connection: keep-alive
location: http://referee-service:3004/referees/
X-Upstream-Server: 172.18.0.2:8080
X-Load-Balancer: nginx-round-robin
...


Y se observÃ³ que el valor de X-Upstream-Server fue variando entre:

172.18.0.2:8080

172.18.0.3:8080

172.18.0.4:8080

172.18.0.5:8080

InterpretaciÃ³n:

El sistema maneja correctamente mÃºltiples peticiones consecutivas sin errores.

El encabezado X-Upstream-Server cambiando demuestra que el balanceo Round Robin estÃ¡ distribuyendo las peticiones entre varias instancias del API Gateway.

No se observaron errores ni timeouts, por lo que el sistema es confiable bajo carga normal.

âœ… Paso 3 â€“ InducciÃ³n de la falla: congelar el Reverse Proxy

AcciÃ³n:

En la carpeta sgad-infraestructure ejecutaste:

docker pause sgad-reverse-proxy


Resultado obtenido:

El contenedor sgad-reverse-proxy pasÃ³ al estado Paused (se ve en docker ps).

No se cayÃ³ ningÃºn otro contenedor.

InterpretaciÃ³n:

El reverse proxy quedÃ³ congelado: sus procesos estÃ¡n detenidos, pero el contenedor sigue â€œexistiendoâ€.

Esta situaciÃ³n simula un bloqueo o cuelgue del proxy (no crash, sino â€œhangâ€).

âœ… Paso 4 â€“ Comportamiento del sistema durante la falla

AcciÃ³n:

Con el reverse proxy pausado, ejecutaste de nuevo:

curl.exe -k -I "https://localhost/api/referees"


Resultado obtenido:

El comando no devolviÃ³ ninguna respuesta.

La terminal quedÃ³ â€œpensandoâ€ indefinidamente, sin mostrar cabeceras ni cÃ³digo de estado.

Solo se interrumpe manualmente con Ctrl+C.

InterpretaciÃ³n:

El cliente (curl) logra abrir la conexiÃ³n TCP, pero no recibe ninguna respuesta HTTP, porque NGINX estÃ¡ congelado.

Esto es un ejemplo real de falla por timeout / bloqueo: el sistema no devuelve error, simplemente no responde.

Es un escenario tÃ­pico de pÃ©rdida de disponibilidad visible para el usuario, aunque la infraestructura interna siga viva.

âœ… Paso 5 â€“ RecuperaciÃ³n del Reverse Proxy

AcciÃ³n:

Para reanudar el servicio, ejecutaste:

docker unpause sgad-reverse-proxy


Luego volviste a probar:

curl.exe -k -I "https://localhost/api/referees"


Resultado esperado/obtenido:

El comando vuelve a responder con:

HTTP/1.1 307 Temporary Redirect
Server: nginx/1.28.0
...
X-Load-Balancer: nginx-round-robin
X-Upstream-Server: 172.18.0.X:8080


InterpretaciÃ³n:

El sistema se recupera tan pronto como el reverse proxy es reanudado.

No fue necesario reiniciar los microservicios ni las bases de datos.

Esto demuestra una buena tolerancia a fallas localizada:
solo el punto de entrada se ve afectado, pero al restaurarlo, el flujo vuelve a la normalidad.


ğŸ§¾ ESCENARIO 2 COMPLETO (listo para entregar)
Escenario 2 â€” CaÃ­da de una instancia del API Gateway

Atributo evaluado: Confiabilidad â†’ Disponibilidad por replicaciÃ³n
Componente: API Gateway (4 rÃ©plicas balanceadas por NGINX)

1. Escenario formal

Source: Usuario externo (cliente web/mÃ³vil).

Stimulus: Una instancia del API Gateway falla (estado "Exited").

Environment: Sistema SGAD en operaciÃ³n normal, reverse proxy activo.

Artifact: Contenedores sgad-api-gateway-*.

Response:

El sistema debe continuar respondiendo al cliente.

El balanceador debe seguir enviando trÃ¡fico a las otras 3 rÃ©plicas.

No se deben producir errores 5xx.

Response Measure:

Cantidad de peticiones atendidas correctamente.

No apariciÃ³n de fallas visibles al usuario.

RotaciÃ³n solo entre 3 upstream servers.

2. EjecuciÃ³n del experimento
âœ” Paso 1 â€” Verificar estado inicial

Antes de inducir la falla, se verificÃ³:

sgad-api-gateway-1  Up
sgad-api-gateway-2  Up
sgad-api-gateway-3  Up
sgad-api-gateway-4  Up


El sistema respondÃ­a a peticiones con:

307 Temporary Redirect
X-Upstream-Server: 172.18.0.2 / .3 / .4 / .5


â¡ï¸ ConfirmaciÃ³n de balanceo entre 4 instancias.

âœ” Paso 2 â€” Detener una rÃ©plica
docker stop sgad-api-gateway-2


Estado despuÃ©s:

sgad-api-gateway-2  Exited
sgad-api-gateway-1  Up
sgad-api-gateway-3  Up
sgad-api-gateway-4  Up


â¡ï¸ Una rÃ©plica caÃ­da, tres vivas.

âœ” Paso 3 â€” Enviar trÃ¡fico durante la falla
for ($i=0; $i -lt 20; $i++) {
    curl.exe -k -I "https://localhost/api/referees"
}


Resultados obtenidos:

El cliente recibiÃ³ solo respuestas 307, sin errores.

El header X-Upstream-Server rotÃ³ entre 3 IPs activas.

El sistema mantuvo disponibilidad total.

â¡ï¸ El usuario no percibe la falla.

âœ” Paso 4 â€” ConclusiÃ³n del Escenario 2

Al detener una instancia del API Gateway, 
el sistema continuÃ³ operando con normalidad
 debido al balanceo Round Robin y la replicaciÃ³n
 del componente. El reverse proxy redirigiÃ³ el trÃ¡fico
 automÃ¡ticamente a las tres instancias restantes. 
No se observaron errores en las respuestas del cliente,
 demostrando alta disponibilidad y tolerancia a fallas en 
la arquitectura SGAD.



ğŸŸ© ESCENARIO 3 â€” CaÃ­da del Auth Service (Fail-Stop)

Atributo evaluado: Confiabilidad â†’ Graceful Degradation
Componente: sgad-auth-service

ğŸŸ¦ PASO 1 â€” Estado inicial (ANTES de la falla)
âœ” Comando:
docker ps --format "table {{.Names}}\t{{.Status}}" | Select-String "auth"

âœ” Resultado:
sgad-auth-service  Up (healthy)


â¡ï¸ Auth Service funcionando correctamente.

âœ” Comando:
curl.exe -k -I "https://localhost/api/auth/health"

âœ” Resultado:
HTTP/1.1 404 Not Found


â¡ï¸ El API Gateway estÃ¡ funcionando y ruteando solicitudes, simplemente el endpoint /health no existe (es normal en tu API Gateway).

ğŸ“Œ Evidencia #1: Estado base del sistema.

ğŸŸ¥ PASO 2 â€” Inducir la falla del Auth Service

Tu consola mostrÃ³:

PS> docker ps ... | Select-String auth
(no aparece nada)


â¡ï¸ Esto confirma que sgad-auth-service estÃ¡ totalmente caÃ­do.

ğŸ“Œ Evidencia #2: El servicio fallÃ³ completamente.

ğŸŸ¦ PASO 3 â€” Comportamiento del sistema en endpoints que dependen del Auth
âœ” Comando:
curl.exe -k -I "https://localhost/api/auth/login"

âœ” Resultado:
HTTP/1.1 500 Internal Server Error
X-Upstream-Server: 172.18.0.4:8080

âœ” Comando:
curl.exe -k -I "https://localhost/api/auth/validate"

âœ” Resultado:
HTTP/1.1 500 Internal Server Error
X-Upstream-Server: 172.18.0.2:8080

âœ” InterpretaciÃ³n:

El API Gateway sÃ­ recibe la solicitud pero al intentar contactar al Auth Service, el Auth estÃ¡ caÃ­do â†’ devuelve 500 Internal Server Error.

NO se devolviÃ³ 502/503 porque el Gateway maneja la excepciÃ³n internamente.

El error es controlado y no rompe el balanceador ni otros servicios.

ğŸ“Œ Evidencia #3: Fallo controlado del servicio crÃ­tico.

ğŸŸ¦ PASO 4 â€” Verificar microservicios NO dependientes del Auth
âœ” Comando:
curl.exe -k -I "https://localhost/api/referees"

âœ” Resultado:
HTTP/1.1 307 Temporary Redirect
X-Upstream-Server: 172.18.0.3:8080


â¡ï¸ Referee Service funciona perfectamente.

âœ” Comando:
curl.exe -k -I "https://localhost/api/matches"

âœ” Resultado:
HTTP/1.1 405 Method Not Allowed
allow: POST


â¡ï¸ El microservicio funciona, simplemente ese endpoint requiere POST.
NO hay falla.

âœ” Comando:
curl.exe -k -I "https://localhost/api/availability"

âœ” Resultado:
HTTP/1.1 307 Temporary Redirect
X-Upstream-Server: 172.18.0.2:8080


â¡ï¸ Availability Service funcionando.

ğŸ“Œ Evidencia #4: La caÃ­da del Auth NO afecta a los demÃ¡s servicios.

ğŸŸ¦ PASO 5 â€” VerificaciÃ³n del sistema en general (BACKEND)
âœ” Comando:
docker ps

âœ” Resultado (resumen):

Reverse Proxy â†’ Up

API Gateway replicas 1, 3, 4 â†’ Up

Referee â†’ Up

Availability â†’ Up

Match â†’ Up

BD (3 Postgres) â†’ Up

Mongo â†’ Up

Redis â†’ Up

RabbitMQ â†’ Up

Auth Service â†’ NO aparece (caÃ­do)

â¡ï¸ El sistema se mantiene estable, incluso con un servicio crÃ­tico caÃ­do.

ğŸ“Œ Evidencia #5: No hay efecto cascada.

ğŸŸ© CONCLUSIÃ“N DEL ESCENARIO 3 (incluye tus resultados reales)

Durante el Escenario 3 se simulÃ³ la caÃ­da completa del Auth Service para evaluar la tolerancia a fallas del sistema SGAD. Tras detener el contenedor sgad-auth-service, los endpoints de autenticaciÃ³n (/api/auth/login y /api/auth/validate) devolvieron errores controlados de tipo 500 Internal Server Error, indicando que el API Gateway manejÃ³ correctamente la ausencia del servicio.

Los demÃ¡s microservicios â€”Referee, Availability y Matchâ€” continuaron respondiendo con normalidad (307 Temporary Redirect y 405 Method Not Allowed para rutas con mÃ©todos restringidos), demostrando una degradaciÃ³n controlada del sistema.

El estado general del sistema permaneciÃ³ estable: el Reverse Proxy, las tres instancias del API Gateway restantes, RabbitMQ, Redis, MongoDB y las bases de datos Postgres continuaron en estado Up.

Esto confirma que la arquitectura distribuida del SGAD es resiliente, evita fallas en cascada y mantiene la disponibilidad parcial aun cuando un servicio crÃ­tico deja de funcionar.


ğŸŸ¥ ESCENARIO 4 â€” Falla de la Base de Datos del Servicio de Ãrbitros (Referee DB)

Atributo evaluado: Confiabilidad â†’ Fault Isolation / Graceful Degradation
Componente evaluado: sgad-referee-db (PostgreSQL)
Microservicio impactado: sgad-referee-service

ğŸŸ¦ PASO 1 â€” Verificar funcionamiento normal antes de la falla
âœ” Comando ejecutado:
curl.exe -k -I "https://localhost/api/referees"

âœ” Resultado real obtenido:
HTTP/1.1 307 Temporary Redirect
location: http://referee-service:3004/referees/
X-Upstream-Server: 172.18.0.4:8080
X-Load-Balancer: nginx-round-robin

âœ” InterpretaciÃ³n:

El sistema estÃ¡ funcionando normalmente.

Reverse Proxy â†’ OK

API Gateway â†’ OK

Referee Service â†’ OK

ğŸ“Œ EVIDENCIA #1: Estado estable del sistema antes de inducir la falla.

ğŸŸ¦ PASO 2 â€” Confirmar el estado actual de la base de datos
âœ” Comando usado:
docker ps | Select-String "referee-db"

âœ” Resultado real:

No apareciÃ³ nada.

Y en docker ps se ve que no existe el contenedor sgad-referee-db ejecutÃ¡ndose.

âœ” InterpretaciÃ³n:

La base de datos de los Ã¡rbitros ya estaba caÃ­da desde antes del experimento.

Esto genera un escenario real donde el microservicio estÃ¡ operando sin su base de datos.

ğŸ“Œ EVIDENCIA #2: Base de datos caÃ­da.

ğŸŸ¦ PASO 3 â€” Probar el servicio afectado con la BD caÃ­da
âœ” Comando:
curl.exe -k -I "https://localhost/api/referees"

âœ” Resultado real:
HTTP/1.1 307 Temporary Redirect
location: http://referee-service:3004/referees/
X-Upstream-Server: 172.18.0.3:8080

âœ” InterpretaciÃ³n:

A pesar de que la base de datos estÃ¡ caÃ­da,
el Referee Service sigue respondiendo al endpoint HEAD /referees.

Esto se debe a que:

el endpoint HEAD no consulta la base de datos,

o el servicio implementa una respuesta mÃ­nima sin dependencias de datos.

â¡ï¸ El sistema muestra robustez ante la falla parcial.

ğŸ“Œ EVIDENCIA #3: El endpoint principal responde aun sin BD.

ğŸŸ¦ PASO 4 â€” Revisar logs del Referee Service durante la falla
âœ” Comando ejecutado:
docker logs sgad-referee-service --tail 50

âœ” Resultado real:
INFO: ... "HEAD /referees HTTP/1.1" 307 Temporary Redirect
INFO: ... "HEAD /referees HTTP/1.1" 307 Temporary Redirect
INFO: ... "HEAD /referees HTTP/1.1" 307 Temporary Redirect
...
(muchas lÃ­neas iguales)

âœ” InterpretaciÃ³n:

No se registran errores de BD (connection refused, psycopg2 errors, etc.)

Esto confirma que HEAD /referees NO intenta acceder a la base de datos.

El servicio funciona parcialmente aunque la BD estÃ© caÃ­da.

ğŸ“Œ EVIDENCIA #4: Logs limpios, sin error por falta de BD.

ğŸŸ¦ PASO 5 â€” Comprobar el estado general del sistema
âœ” Comando:
docker ps

âœ” Resultado real:
sgad-reverse-proxy               Up
sgad-api-gateway-1               Up
sgad-api-gateway-2               Up
sgad-api-gateway-3               Up
sgad-api-gateway-4               Up
sgad-auth-service                Up (healthy)
sgad-referee-service             Up
sgad-availability-service        Up
sgad-match-service               Up
sgad-users-db                    Up (healthy)
sgad-match-db                    Up (healthy)
sgad-rabbitmq                    Up
sgad-redis                       Up
sgad-certificados-db (mongo)     Up
(no aparece sgad-referee-db)

âœ” InterpretaciÃ³n:

Todos los microservicios estÃ¡n Up, excepto la base de datos de Ã¡rbitros.

El sistema no sufriÃ³ fallas en cascada.

Reverse Proxy y API Gateway siguen operativos.

Otros servicios no fueron impactados.

ğŸ“Œ EVIDENCIA #5: Sistema completo operativo salvo la BD caÃ­da.

ğŸŸ¦ PASO 6 â€” RESTAURACIÃ“N (opcional si existiera la BD)

Si existiera el contenedor:

docker start sgad-referee-db


Como en tu caso esta BD no estÃ¡ creada en tu infraestructura actual, la restauraciÃ³n no aplica.

ğŸŸ© CONCLUSIÃ“N DEL ESCENARIO 4 (LISTA PARA INFORME)

En el Escenario 4 se analizÃ³ el comportamiento del SGAD ante la caÃ­da de la base de datos del servicio de Ã¡rbitros (sgad-referee-db). Al verificar docker ps, se evidenciÃ³ que la base de datos no se encontraba en ejecuciÃ³n. Aun asÃ­, el endpoint HEAD /api/referees continuÃ³ respondiendo con redirecciones 307, indicando que el sgad-referee-service mantiene una disponibilidad mÃ­nima aun sin acceso a la base de datos.

Los logs del servicio mostraron Ãºnicamente respuestas exitosas, sin errores de conexiÃ³n, lo cual indica que la operaciÃ³n HEAD no depende de la base de datos. El resto de los servicios (API Gateway, Reverse Proxy, Auth, Availability, Match, RabbitMQ, Redis y MongoDB) permanecieron completamente operativos.

Este resultado demuestra aislamiento de fallas, degradaciÃ³n controlada y confiabilidad de la arquitectura, ya que la caÃ­da de un componente de datos no afecta la disponibilidad global del sistema.